<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Potterhe&#39;s Site</title>
    <link>https://potterhe.github.io/posts/</link>
    <description>Recent content in Posts on Potterhe&#39;s Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 20 Mar 2020 23:37:00 +0800</lastBuildDate>
    
	<atom:link href="https://potterhe.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Istio 1.5.0 实战：授权(Authorization)</title>
      <link>https://potterhe.github.io/posts/istio-1.5-in-action-authorization/</link>
      <pubDate>Fri, 20 Mar 2020 23:37:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/istio-1.5-in-action-authorization/</guid>
      <description>在进行本节练习之前，先按照官方用例 Authorization on Ingress Gateway 完成 httpbin 的部署，httpbin 这个应用真的很好用。
$ kubectl apply -f samples/httpbin/httpbin.yaml -n test-mesh $ kubectl apply -f samples/httpbin/httpbin-gateway.yaml -n test-mesh 从公网访问 http://your-lb-ip/，预期显示的是 httbin 的 swagger ui。/ip，/anything，/headers 等 restful api 会给我们调试验证带来极大的帮助。如下面的 /anything 请求，其行为是 “Returns anything that is passed to request”。
$ curl http://your-lb-ip/anything { &amp;#34;args&amp;#34;: {}, &amp;#34;data&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;files&amp;#34;: {}, &amp;#34;form&amp;#34;: {}, &amp;#34;headers&amp;#34;: { &amp;#34;Accept&amp;#34;: &amp;#34;*/*&amp;#34;, &amp;#34;Content-Length&amp;#34;: &amp;#34;0&amp;#34;, &amp;#34;Host&amp;#34;: &amp;#34;your-lb-ip&amp;#34;, &amp;#34;User-Agent&amp;#34;: &amp;#34;curl/7.54.0&amp;#34;, &amp;#34;X-B3-Parentspanid&amp;#34;: &amp;#34;7d83460f9c0bb319&amp;#34;, &amp;#34;X-B3-Sampled&amp;#34;: &amp;#34;1&amp;#34;, &amp;#34;X-B3-Spanid&amp;#34;: &amp;#34;e2a3c9482108d0b5&amp;#34;, &amp;#34;X-B3-Traceid&amp;#34;: &amp;#34;149ddcb0970898197d83460f9c0bb319&amp;#34;, &amp;#34;X-Envoy-Internal&amp;#34;: &amp;#34;true&amp;#34;, &amp;#34;X-Forwarded-Client-Cert&amp;#34;: &amp;#34;By=spiffe://cluster.</description>
    </item>
    
    <item>
      <title>基于 Kubeapps 的应用管理、发布系统</title>
      <link>https://potterhe.github.io/posts/a-kubeapps-based-application-deploying-and-managing-system/</link>
      <pubDate>Sat, 14 Mar 2020 11:52:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/a-kubeapps-based-application-deploying-and-managing-system/</guid>
      <description>在引入 Kubernetes 时，我们需要提供对应的 CI/CD 方案。
需求&amp;amp;痛点  私有的统一认证，授权。角色，权限管控。 审计合规。主要包括测试人员发版，发版有历史，可审计追溯。 多云，多集群。 功能产品化。如：发版，扩容，回滚，下线。 “回滚”机制要求“编排文件”也需要用版本控制工具管理起来，版本化。  选型  rancher drone spinnaker helm 生态  为什么选择 Kubeapps Kubeapps
在整理这篇的时候，特意确认了 kubeapps 的最新架构，一些组件的名字已经发生了改变。   需求描述中的“编排文件版本化”，这正是 helm 的特性，虽然社区有其它的解决方案，但 helm 目前仍是那个应用最广泛的，成熟且有丰富的生态。 kubeapps 基于 helm。 kubeapps 是产品化的，而不是一个“轮子”。 封装良好的 api，极容易复用和扩展。这些 api 包括：整合了chartmuseum，并提供了chartsvc；tiller-proxy api；apprepository api；搜索 api，基于mongo的缓存，定时同步。 权限透明。它的权限完全是基于 Kubernetes 的 RBAC 机制，是透明的。 前端 React。  chartmuseum chartmuseum 是 helm 的 charts 仓库。
 无状态 几乎支持所有云的对象存储作为后端，简单，安全。  由于我们的业务主要是在腾讯云，我们希望用其 cos 服务作为存储 backend。我们为社区提供了两个pr，完善了 chartmuseum 对腾讯云 cos 的支持。</description>
    </item>
    
    <item>
      <title>Istio 1.5.0 实战：“体验”监控，观测</title>
      <link>https://potterhe.github.io/posts/istio-1.5-in-action-setup-addon-components/</link>
      <pubDate>Thu, 12 Mar 2020 12:52:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/istio-1.5-in-action-setup-addon-components/</guid>
      <description>本文将部署 Istio 的监控，观测附加组件 grafana、tracing、kiali 。标题中使用了“体验”这个词，所以本文所做的练习只是用于体验，切不可用于生产，且行文的表述都是基于“体验”这个前提，体验完后，请务必清理。这种建议主要基于以下考虑:
 安全。在没有应用“认证”、“授权”机制前，这些服务裸奔在公网是危险的，即使有的系统有帐密。 存储。这些服务都是有状态的，涉及状态数据的存储，如：promethues、grafana、kiali、tracing。这些 Pod 一旦重建，数据就会丢失。一般地，需要我们适配 k8s 标准化的存储机制，如：storageClass、pv、pvc等。  组件部署 grafana 在 override 文件中添加 addonComponents.grafana.enabled=true 项后，应用到集群。
apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system spec: profile: default addonComponents: grafana: enabled: true values: gateways: istio-ingressgateway: serviceAnnotations: # https://cloud.tencent.com/document/product/457/18210 service.kubernetes.io/qcloud-loadbalancer-backends-label: company.com/ingressgateway=true nodeSelector: company.com/ingressgateway: true ports: # 公网lb控制暴露的端口. - name: http2 port: 80 targetPort: 80 - name: https port: 443 查看现场，预期会增加以下对象
$ kubectl get all -n istio-system NAME READY STATUS RESTARTS AGE pod/grafana-5dc9c87cdf-j6ml8 1/1 Running 0 3d9h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana ClusterIP 172.</description>
    </item>
    
    <item>
      <title>Istio 1.5.0 实战：安装</title>
      <link>https://potterhe.github.io/posts/istio-1.5-in-action-setup/</link>
      <pubDate>Wed, 11 Mar 2020 23:59:50 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/istio-1.5-in-action-setup/</guid>
      <description>istio 1.5.0 于北京时间2020年3月6日发布，该版本在架构上有很大的变化(Istio 1.5 新特性解读)，做出这些改变的原因及其重大意义这里不做赘述，有大量的文章做了阐述，本文聚焦于探索 istio-1.5.0 的行为。文中的练习是在腾讯云的容器服务 tke-1.16.3 上进行的，会涉及到一些云平台相关的实现。
安装istio 使用 istioctl 安装是官方推荐的方式(Customizable Install with Istioctl)，helm 安装方式已经被标记为 deprecated，且已经不支持 helm3。istioctl 安装实践，一般选择一个profile（生产环境，社区推荐基于 default），然后用自定义的值进行覆盖，一般而言，我们只需要编写这个 override 文件(这里的文件名是 profile-override-default.yaml)，不需要修改 charts （当然charts是可以被修改的）。
apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system spec: profile: default 另一面，override 文件中的项，以及项的节点层次怎么找呢？建议两种方式：1，通过 istioctl profile dump default 命令可以打印一些，但不够全；2，查看 charts （install/kubernetes/operator/charts/） 里各子项目的 values 文件定义，这种方式最可靠，对值的行为还可以辅以 templates 文件更深入的了解，推荐这种。
有了 override 文件后，就可以使用下面的命令生成 manifest，manifest 是 k8s 的定义。建议把 override 文件和生成的 manifest 文件都通过版本控制工具维护起来，在每次对 override 操作后，对比前后的差异。
$ istioctl manifest generate -f profile-override-default.yaml &amp;gt; manifest-override-default.</description>
    </item>
    
    <item>
      <title>Kubernetes &#34;no route to host&#34;问题</title>
      <link>https://potterhe.github.io/posts/k8s-no-route-to-host/</link>
      <pubDate>Fri, 13 Dec 2019 23:59:59 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/k8s-no-route-to-host/</guid>
      <description>我们在使用腾讯云容器服务(tke)的过程中，遇到&amp;quot;no route to host&amp;quot;问题，这里记录为运维日志。
环境  tke 1.12.4(1.14.3) 托管集群. 节点操作系统：ubuntu16.04.1 LTSx86_64 kube-proxy ipvs模式 /usr/bin/kube-proxy &amp;ndash;proxy-mode=ipvs &amp;ndash;ipvs-min-sync-period=1s &amp;ndash;ipvs-sync-period=5s &amp;ndash;ipvs-scheduler=rr &amp;ndash;masquerade-all=true &amp;ndash;kubeconfig=/etc/kubernetes/kubeproxy-kubeconfig &amp;ndash;hostname-override=172.21.128.111 &amp;ndash;v=2 运行时：Docker version 18.06.3-ce, build d7080c1  排查过程 请详见tke团队roc的文章Kubernetes 疑难杂症排查分享: 诡异的 No route to host
其它找到的一些有用的文章  https://engineering.dollarshaveclub.com/kubernetes-fixing-delayed-service-endpoint-updates-fd4d0a31852c https://fuckcloudnative.io/posts/kubernetes-fixing-delayed-service-endpoint-updates/ 中文版本 五元组：源IP地址，源端口，目的IP地址，目的端口，和传输层协议这五个量组成的一个集合  </description>
    </item>
    
    <item>
      <title>kube-proxy的ipvs模式udp转发规则过期问题</title>
      <link>https://potterhe.github.io/posts/k8s-ipvs-udp-rule-expire-5min/</link>
      <pubDate>Tue, 03 Dec 2019 23:59:59 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/k8s-ipvs-udp-rule-expire-5min/</guid>
      <description>我们在使用腾讯云容器服务(tke)的过程中，遭遇了kube-proxy的ipvs模式udp转发规则过期问题，过程记录。
环境  tke 1.12.4 托管集群. 节点操作系统：ubuntu16.04.1 LTSx86_64 kube-proxy ipvs模式 /usr/bin/kube-proxy &amp;ndash;proxy-mode=ipvs &amp;ndash;ipvs-min-sync-period=1s &amp;ndash;ipvs-sync-period=5s &amp;ndash;ipvs-scheduler=rr &amp;ndash;masquerade-all=true &amp;ndash;kubeconfig=/etc/kubernetes/kubeproxy-kubeconfig &amp;ndash;hostname-override=172.21.128.111 &amp;ndash;v=2 运行时：Docker version 18.06.3-ce, build d7080c1  操作和现象  目标节点：172.21.128.109, 该节点有coredns, coredns的svc ClusterIP: 172.23.127.235。执行封锁后，执行drain操作。操作后的现象：业务大量报错“getaddrinfo failed: Name or service not known (10.010s)”,持续约8分钟. 和腾讯云的伙伴复盘时关注dns的变化.操作后会看到新的pod在172.21.128.111节点生成，在集群的任意节点上查看ipvs规则,发现tcp规则已更新成新podip，但udp规则还是老的podip。  # kubectl drain 172.21.128.109 # kubectl get pod -n kube-system -o wide|grep dns coredns-568cfc555b-4vdgk 1/1 Running 0 66s 172.23.3.41 172.21.128.111 &amp;lt;none&amp;gt; coredns-568cfc555b-7zkfz 1/1 Running 0 77d 172.23.0.144 172.21.128.10 &amp;lt;none&amp;gt; # ipvsadm -Ln|grep -A2 172.</description>
    </item>
    
    <item>
      <title>cdh-5.13 quickstart vm 使用笔记</title>
      <link>https://potterhe.github.io/posts/cdh-quick-start-vm/</link>
      <pubDate>Sat, 06 Jul 2019 23:59:59 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/cdh-quick-start-vm/</guid>
      <description>下载和导入虚拟机.  下载 cdh-quickstart-vm 导入  vm初始信息收集 [cloudera@quickstart ~]$ uname -a Linux quickstart.cloudera 2.6.32-573.el6.x86_64 #1 SMP Thu Jul 23 15:44:03 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux $ cat /etc/issue CentOS release 6.7 (Final) Kernel \r on an \m vm里的cdh服务当前是使用操作系统的init系统管理的，而不是&amp;quot;Cloudera Manager&amp;rdquo;
By default, the Cloudera QuickStart VM run Cloudera&#39;s Distribution including Apache Hadoop (CDH) under Linux&#39;s service and configuration management. If you wish to migrate to Cloudera Manager, you must run one of the following commands.</description>
    </item>
    
    <item>
      <title>Spark 学习笔记</title>
      <link>https://potterhe.github.io/posts/spark-learning-note/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/spark-learning-note/</guid>
      <description>目标  概念 本地开发  本地环境(Mac OS X)  下载  export JAVA_HOME=$(/usr/libexec/java_home -v 1.8) export SPARK_HOME=&amp;#34;$HOME/opt/spark-2.3.2-bin-hadoop2.6&amp;#34; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH export PATH=$SPARK_HOME/bin:$PATH python环境 virtualenv -p python3 spark-python3 source spark-python3/bin/activate pip install pyspark deactivate(仅从当前venv环境脱离时执行) helloworld ./bin/spark-submit examples/src/main/python/wordcount.py README.md spark-shell IDE(PyCharm)  首选项-Project Interpreter helloworld解读  RDD  partition 转换 行动 lazy evaluation shuffle  并行化  partition 与并发 worker,executor,task,job   Spark Streaming, DStream socket ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999  web ui, http://localhost/4040  kafka  https://search.</description>
    </item>
    
    <item>
      <title>Spark调研笔记</title>
      <link>https://potterhe.github.io/posts/spark-note/</link>
      <pubDate>Thu, 21 Feb 2019 23:59:59 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/spark-note/</guid>
      <description>理论  streaming 101 streaming 102  spark  子雨大数据之Spark入门教程(Python版) RDD、DataFrame和DataSet的区别 Spark Streaming 不同Batch任务可以并行计算么？ Spark Streaming 管理 Kafka Offsets 的方式探讨 Spark Streaming容错性和零数据丢失 Structured Streaming Programming Guide结构化流编程指南 是时候放弃 Spark Streaming, 转向 Structured Streaming 了  选项  spark.io.compression.codec snappy (lz4依赖冲突) spark.streaming.concurrentjobs (spark streaming实时大数据分析4.4.4) spark.streaming.receiver.writeaheadlog.enable (spark streaming实时大数据分析5.6节) spark.sql.shuffle.partitions (default 200, 在单节点测试时,会造成极大的延迟).  pyspark  Improving PySpark performance: Spark Performance Beyond the JVM Python最佳实践指南  本地环境搭建 export JAVA_HOME=$(/usr/libexec/java_home -v 1.8) export SPARK_HOME=&amp;#34;$HOME/opt/spark-2.3.2-bin-hadoop2.6&amp;#34; export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.</description>
    </item>
    
    <item>
      <title>收藏的一些论文</title>
      <link>https://potterhe.github.io/posts/paper-collect/</link>
      <pubDate>Sat, 09 Feb 2019 23:59:50 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/paper-collect/</guid>
      <description>分布式系统  寻找一种易于理解的一致性算法（扩展版） (raft) Dapper，大规模分布式系统的跟踪系统  大数据  streaming 101 streaming 102(翻译)  </description>
    </item>
    
    <item>
      <title>Mac OS X 10.9 自带 php-fpm 的配置使用和扩展安装</title>
      <link>https://potterhe.github.io/posts/macosx-conf-preloaded-php-fpm/</link>
      <pubDate>Sat, 11 Jan 2014 00:00:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/macosx-conf-preloaded-php-fpm/</guid>
      <description>Mac OS X 10.9 自带有 php-fpm，本文把预装的 php-fpm 配置起来。
直接运行，有报错找不到配置文件。
$ php-fpm [11-Jan-2014 16:03:03] ERROR: failed to open configuration file &amp;#39;/private/etc/php-fpm.conf&amp;#39;: No such file or directory (2) [11-Jan-2014 16:03:03] ERROR: failed to load configuration file &amp;#39;/private/etc/php-fpm.conf&amp;#39; [11-Jan-2014 16:03:03] ERROR: FPM initialization failed 可以在 /private/etc/ 目录下生成配置文件，需要 root 权限(sudo) 或者在普通用户有权限的目录里放置配置文件，通过 --fpm-config 参数指定配置文件的位置，如下：
$ cp /private/etc/php-fpm.conf.default /usr/local/etc/php-fpm.conf $ php-fpm --fpm-config /usr/local/etc/php-fpm.conf [11-Jan-2014 16:10:49] ERROR: failed to open error_log (/usr/var/log/php-fpm.log): No such file or directory (2) [11-Jan-2014 16:10:49] ERROR: failed to post process the configuration [11-Jan-2014 16:10:49] ERROR: FPM initialization failed 错误信息显示：不能正确的打开”日志“文件，原因是默认在 /usr/var 目录下工作，可以修改配置文件指定正确的日志文件路径。修改 /usr/local/etc/php-fpm.</description>
    </item>
    
    <item>
      <title>Discuz!X集群部署的系统方案和优化方式</title>
      <link>https://potterhe.github.io/posts/discuz-cluster-optimize/</link>
      <pubDate>Sun, 22 Sep 2013 00:00:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/discuz-cluster-optimize/</guid>
      <description>多 web 部署时，面临的核心问题是 web 服务器间的数据共享和同步。就数据存储的方式而言，Discuz 数据包含两部分：一部分存储在 MySQL 数据库中（用户、帖子等文本类、结构化的数据），一部分存储在文件系统（附件、缓存文件等）。其中存储在 MySQL 中的数据可以方便地在多服务器间共享，扩展和冗余也已经有比较成熟的方案。这里我们主要讨论 Discuz 文件类型的数据，部分涉及到多台服务器的内容。
数据梳理 Discuz 文件类型的数据都存储于 DISCUZ_ROOT/data 目录，各目录主要功能如下：
   目录 数据说明     data/attachment 附件类   data/log 运行日志   data/cache 配置参数类缓存文件（默认是sql，配置参数通过pre_common_syscache表缓存）、CSS缓存、部分JS缓存   data/template 模板缓存   data/threadcache 论坛页面缓存（针对游客的优化）    DISCUZ_ROOT/data目录下有几个重要的文件（文件锁）
 data/install.lock，安装程序锁定。如果该文件存在，DISCUZ_ROOT/install/ 中的安装程序不能执行。 data/sendmail.lock， 发送邮件锁。Discuz 默认通过类似 home.php?mod=misc&amp;amp;ac=sendmail&amp;amp;rand=1379315574 这个隐藏页面调用，由用户的浏览行为触发邮件发送流程（浏览器侧用一个300秒的 cookie 控制频率，服务器侧通过 sendmail.lock 文件的 mtime 控制频率5秒）。如果可以控制服务器，应该优化掉这个机制。 data/updatetime.lock， 某管理后台使用的锁。 data/update.lock， 系统升级锁。执行版本升级程序（如x2升级到x3）时，会生成这个文件锁。  下面这些功能会涉及到多 web 服务器间的数据共享和同步，默认 Discuz 通过 MySQL 实现。</description>
    </item>
    
    <item>
      <title>Discuz!顶置贴、帖子列表优化建议</title>
      <link>https://potterhe.github.io/posts/discuz-threads-list-optimize/</link>
      <pubDate>Mon, 22 Jul 2013 00:00:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/discuz-threads-list-optimize/</guid>
      <description>顶置贴的存储 表 pre_forum_thread 字段 displayorder
4 多版块顶置。Disczu!7.2引入的一个特性，操作入口在“论坛”－“版块/群组顶置” 3 3级置顶、全局顶置 2 2级置顶、分区顶置 1 1级置顶、版块顶置 0 正常 -1 回收站 -2 审核中 -3 审核忽略 -4 草稿   顶置2、3类的tid会被保存在pre_common_syscache的cname=&amp;quot;globalstick&amp;quot;记录中 顶置2 $_G[&#39;cache&#39;][&#39;globalstick&#39;][&#39;categories&#39;][分区ID] 顶置3 $_G[&amp;lsquo;cache&amp;rsquo;][&amp;lsquo;globalstick&amp;rsquo;][&amp;lsquo;global&amp;rsquo;][&amp;lsquo;tids&amp;rsquo;] 多版块顶置数据（4）被保存在pre_common_syscache的cname=&amp;quot;forumstick&amp;quot;记录中 $_G[&amp;lsquo;cache&amp;rsquo;][&amp;lsquo;forumstick&amp;rsquo;][版块ID]  帖子列表页对顶置贴的显示逻辑。 Discuz!通过下面的SQL查询检索上面3种置顶帖子数据。
第一页
SELECT * FROM pre_forum_thread WHERE `tid` IN(&#39;3&#39;,&#39;13&#39;,&#39;12&#39;,&#39;5&#39;) AND `displayorder` IN(&#39;2&#39;,&#39;3&#39;,&#39;4&#39;) ORDER BY displayorder DESC, lastpost DESC LIMIT 20 Using where; Using filesort 第二页
SELECT * FROM pre_forum_thread WHERE `tid` IN(&#39;3&#39;,&#39;13&#39;,&#39;12&#39;,&#39;5&#39;) AND `displayorder` IN(&#39;2&#39;,&#39;3&#39;,&#39;4&#39;) ORDER BY displayorder DESC, lastpost DESC LIMIT 20, 20 Using where; Using filesort 可以看到，即使第二页没有需要显示的顶置贴，但仍然会执行查询，这里可以做个小小的优化，计算一下IN（）表达式中的tid的数量。 然后是排序部分，如果条件允许的话，建议通过PHP脚本来排序。是否使用这一项优化，主要受版块中顶置贴的数量影响。如果顶置贴数量较多，必须要通过分页来显示，就需要在数据库中排序后，才能确定在指定页面要显示的顶置贴是哪些。不过按通常的情况来看，版块中顶置贴的数量都还是比较少，很少会出现顶置贴翻页的情况。</description>
    </item>
    
    <item>
      <title>shell算术展开、按位运算</title>
      <link>https://potterhe.github.io/posts/shell-bit-computing/</link>
      <pubDate>Fri, 05 Jul 2013 00:00:00 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/shell-bit-computing/</guid>
      <description>《shell脚本学习指南》6.1.3节描述了 shell 的算术展开，其支持的运算与C语言差不多，语法 $((...))
$ echo $(( 3 * 4 )) 12 在某些场景特别方便，可以免去写程序的烦琐，如验证某些运算。 下面是验证《深入理解计算系统》练习题2.12的场景
表达式 ~0 将生成一个全1的掩码，不管机器的字大小是多少，可移植。
$ printf &amp;#34;%x\n&amp;#34; $(( ~0 )) ffffffffffffffff $ printf &amp;#34;%#x\n&amp;#34; $(( ~0 )) 0xffffffffffffffff 上面的测试显示，shell中，0按位取反后的值是64位的。 shell 的 printf 命令前导字符打印：《shell脚本学习指南》表7－4：printf 的标志中描述了格式参数中&amp;rdquo;#&amp;ldquo;号的意义，&amp;quot;#&amp;ldquo;可以用以输出前导&amp;quot;0x&amp;rdquo;(16进制)、&amp;ldquo;0&amp;rdquo;(8进制)
x &amp;amp; 0xFF 生成一个由x的最低有效字节组成的值
$ printf &amp;#34;%#x\n&amp;#34; $(( 0x89ABCDEF &amp;amp; 0xFF )) 0xef $ printf &amp;#34;%#.8x\n&amp;#34; $(( 0x89ABCDEF &amp;amp; 0xFF )) 0x000000ef 以下x = 0x87654321 A.x的最低有效字节，其他位均置为0
$ printf &amp;#34;%#.8x\n&amp;#34; $(( 0x87654321 &amp;amp; 0xFF )) 0x00000021 $ printf &amp;#34;%#.</description>
    </item>
    
    <item>
      <title>cacti源码分析－数据采集</title>
      <link>https://potterhe.github.io/posts/cacti-source-code-insight/</link>
      <pubDate>Sat, 25 May 2013 23:59:50 +0800</pubDate>
      
      <guid>https://potterhe.github.io/posts/cacti-source-code-insight/</guid>
      <description>cacti用于监控系统的各项运行指标，提供了交互界面和图表，是一个整合工具集，它完成两个核心任务： 1)指标数据的采集，2) 将数据通过数图进行展示。其中图表的绘制、图表数据的存储是通过rrdtool工具实现的，《RRDtool简体中文教程》对rrdtool工具进行了介绍，是很好的资料。本文分析指标数据采集的实现。
如何获取目标数据 我们需要按照目标数据的暴露方式去采集相应的数据. 基于主机(host)的数据，如：系统负载，网卡流量，磁盘IO，TCP连接等已通过SNMP标准化，需要使用SNMP方式获取。应用级的数据要按照应用暴露数据的方式去获取,如: 如果要监控nginx（stub_status）数据,该项数据是通过http方式暴露，需使用http获取数据;如果要监控mysql-server（show status）数据，需要可以连接到mysql服务器，并有权限打印数据。
cacti由cron驱动 cacti不是一个daemon进程，它由cron驱动。通常我们需要配置如下的cron: 下面的典型配置为每5分钟运行一次cacti的轮换数据进程。
*/5 * * * * /PATH/TO/php /PATH/TO/cacti/poller.php &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 cron任务的运行频率，影响cacti采集数据周期，但cron的运行频率，并不是cacti最终的采集频率。举例来说，如果cron的运行频率为每5分钟触发一次，但我们希望每分钟采集一次数据，cacti是可以做到的，这要求我们将cron的运行频率正确的配置到cacti，这样cacti会在一次cron进程生命周期内，尽可能的按照预期的频率，进行多次数据采集。
实现 涉及 $cron_interval，$poller_interval 两项参数，比如cron的周期是5分钟，poller周期是1分钟，则cron触发的poller.php进程，要负责安排5次数据轮询，以满足poller粒度。
$poller_runs = intval($cron_interval / $poller_interval); define(&amp;#34;MAX_POLLER_RUNTIME&amp;#34;, $poller_runs * $poller_interval - 2); // poller.php进程的最大运行时间（比计划任务周期少2秒） poller进程(poller.php) poller进程由cron启动，在其生命周期内，负责编排所有的数据采集任务.
任务初始化 poller进程启动后，在进行一系列cacti的初始化后，从系统中检索出数据采集任务集，然后将它们持久化到数据库(poller_output表)中,每一项数据指标一条记录。
并发控制 cacti提供了两种并发模型来提升数据采集的效率，cmd.php和spine。其中cmd.php是多进程模型，用php语言实现; spine是多线程模型，具体实现不详。这里我们只讨论cmd.php方式的并发。下面的引文来自cacti的配置界面说明，大意是说，当使用cmd.php抓取数据时，可以通过增加进程数来提高性能（多进程模型）；当使用spine时，应该通过增加“Maximum Threads per Process”的值，提升性能（多线程模型）。
 &amp;ldquo;The number of concurrent processes to execute. Using a higher number when using cmd.php will improve performance. Performance improvements in spine are best resolved with the threads parameter&amp;rdquo;</description>
    </item>
    
  </channel>
</rss>